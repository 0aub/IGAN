import torch
import torch.nn.functional as F
from torch.autograd import Variable, grad

def map_labels(label, classes):
    """
    Map labels to continuous values from 0 to number of classes - 1.
    Args:
        label (torch.Tensor): The original labels.
        classes (torch.Tensor): Unique classes in the dataset.
    Returns:
        torch.Tensor: Mapped labels.
    """
    mapped_label = torch.LongTensor(label.size())
    for i in range(classes.size(0)):
        mapped_label[label == classes[i]] = i
    return mapped_label

def sample(data, input_res, input_att, input_label, batch_size):
    """
    Samples a batch of data for training.

    Args:
        data (DataLoader): The DataLoader instance providing the data.
        input_res (torch.Tensor): Tensor to store sampled features.
        input_att (torch.Tensor): Tensor to store sampled attributes.
        input_label (torch.Tensor): Tensor to store sampled labels.
        batch_size (int): The size of the batch to sample.
    """
    batch_feature, batch_label, batch_att = data.next_batch(batch_size)
    input_res.copy_(batch_feature)
    input_att.copy_(batch_att)
    input_label.copy_(map_labels(batch_label, data.seenclasses))

def generate_synthetic_features(netG, classes, attribute, num, resSize, attSize, nz):
    """
    Generates synthetic features using the generator network.

    Args:
        netG (torch.nn.Module): The generator network.
        classes (torch.Tensor): Tensor of class indices for which to generate features.
        attribute (torch.Tensor): Tensor of class attributes.
        num (int): Number of synthetic instances to generate per class.
        resSize (int): Size of the resulting feature vector.
        attSize (int): Size of the attribute vector.
        nz (int): Size of the noise vector.

    Returns:
        Tuple[torch.FloatTensor, torch.LongTensor]: Tuple containing the synthetic features and their labels.
    """
    nclass = classes.size(0)
    syn_feature = torch.FloatTensor(nclass * num, resSize)
    syn_label = torch.LongTensor(nclass * num)
    syn_att = torch.FloatTensor(num, attSize).cuda()
    syn_noise = torch.FloatTensor(num, nz).cuda()

    for i in range(nclass):
        iclass = classes[i]
        iclass_att = attribute[iclass]
        syn_att.copy_(iclass_att.repeat(num, 1))
        syn_noise.normal_(0, 1)
        with torch.no_grad():
            output = netG(syn_noise, syn_att)
        syn_feature.narrow(0, i * num, num).copy_(output.data.cpu())
        syn_label.narrow(0, i * num, num).fill_(iclass)
    return syn_feature, syn_label

def calc_gradient_penalty(netD, real_data, fake_data, input_att, batch_size, lambda1):
    """
    Calculates the gradient penalty for training with WGAN-GP.

    Args:
        netD (torch.nn.Module): The discriminator or critic network.
        real_data (torch.Tensor): Batch of real data.
        fake_data (torch.Tensor): Batch of fake data generated by the generator.
        input_att (torch.Tensor): Batch of attribute data.
        batch_size (int): Size of the batch.
        lambda1 (float): Gradient penalty coefficient.

    Returns:
        torch.Tensor: Calculated gradient penalty.
    """
    alpha = torch.rand(batch_size, 1)
    alpha = alpha.expand(real_data.size()).cuda()
    interpolates = alpha * real_data + ((1 - alpha) * fake_data)
    interpolates = Variable(interpolates, requires_grad=True)
    disc_interpolates = netD(interpolates, input_att)
    gradients = grad(outputs=disc_interpolates, inputs=interpolates, grad_outputs=torch.ones(disc_interpolates.size()).cuda(), create_graph=True, retain_graph=True, only_inputs=True)[0]
    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * lambda1
    return gradient_penalty

def class_scores_for_loop(embed, input_label, relation_net, data, opt):
    """
    Computes class scores in a loop to save GPU memory.

    Args:
        embed (torch.Tensor): Embedded features.
        input_label (torch.Tensor): Labels of the input features.
        relation_net (torch.nn.Module): The relation network to compute scores.
        data (DataLoader): DataLoader providing dataset information.
        opt (argparse.Namespace): Options containing configuration parameters.

    Returns:
        torch.Tensor: Loss computed from the class scores.
    """
    all_scores = torch.FloatTensor(embed.shape[0], opt.seen_classes).cuda()
    for i, i_embed in enumerate(embed):
        expand_embed = i_embed.repeat(opt.seen_classes, 1)  # Replicate for each class
        all_scores[i] = (torch.div(relation_net(torch.cat((expand_embed, data.attributes_seen.cuda()), dim=1)), opt.class_temperature).squeeze())
    score_max, _ = torch.max(all_scores, dim=1, keepdim=True)
    scores_norm = all_scores - score_max.detach()  # Normalize scores
    mask = F.one_hot(input_label, num_classes=opt.seen_classes).float().cuda()  # Create mask for one-hot labels
    exp_scores = torch.exp(scores_norm)  # Exponential for softmax
    log_scores = scores_norm - torch.log(exp_scores.sum(1, keepdim=True))  # Log softmax
    cls_loss = -((mask * log_scores).sum(1) / mask.sum(1)).mean()  # Compute loss
    return cls_loss

def class_scores_in_matrix(embed, input_label, relation_net, data, opt):
    """
    Computes class scores using matrix operations for speed but at the cost of higher GPU memory usage.

    Args:
        embed (torch.Tensor): Embedded features.
        input_label (torch.Tensor): Labels of the input features.
        relation_net (torch.nn.Module): The relation network to compute scores.
        data (DataLoader): DataLoader providing dataset information.
        opt (argparse.Namespace): Options containing configuration parameters.

    Returns:
        torch.Tensor: Loss computed from the class scores.
    """
    expand_embed = embed.unsqueeze(dim=1).repeat(1, opt.seen_classes, 1).reshape(embed.shape[0] * opt.seen_classes, -1)
    expand_att = data.attributes_seen.unsqueeze(dim=0).repeat(embed.shape[0], 1, 1).reshape(embed.shape[0] * opt.seen_classes, -1).cuda()
    all_scores = torch.div(relation_net(torch.cat((expand_embed, expand_att), dim=1)), opt.class_temperature).reshape(embed.shape[0], opt.seen_classes)
    score_max, _ = torch.max(all_scores, dim=1, keepdim=True)
    scores_norm = all_scores - score_max.detach()
    mask = F.one_hot(input_label, num_classes=opt.seen_classes).float().cuda()
    exp_scores = torch.exp(scores_norm)
    log_scores = scores_norm - torch.log(exp_scores.sum(1, keepdim=True))
    cls_loss = -((mask * log_scores).sum(1) / mask.sum(1)).mean()
    return cls_loss